import os
import getpass
from typing import List, Optional
from pydantic import BaseModel, Field

# LangSmith ê´€ë ¨ ì„í¬íŠ¸
from langsmith import Client, evaluate

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# Kimnote í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from retriever_builder import build_retriever
from graph_workflow import create_rag_graph

# -----------------------------------------------------------------------------
# 1. í™˜ê²½ ì„¤ì • ë° ë°ì´í„°ì…‹ ì¤€ë¹„
# -----------------------------------------------------------------------------

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")
if "LANGCHAIN_API_KEY" not in os.environ:
    os.environ["LANGCHAIN_API_KEY"] = getpass.getpass("LangSmith API Key:")

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "KW-RAG"

# í‰ê°€ìš© LLM (ì±„ì ì)
# ì½”ë“œê°€ ì´ë¯¸ ê²€ì¦ëœ ChatOpenAIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
eval_llm = ChatOpenAI(model="gpt-4o", temperature=0)

# í‰ê°€ ë°ì´í„°ì…‹
EVAL_DATASET = [
    {
        "inputs": {"question": "ì´ ë…¼ë¬¸ì˜ ì£¼ìš” ê³µí—Œ(contributions)ì€ ë¬´ì—‡ì¸ê°€ìš”?"},
        "outputs": {"answer": "ì¬ë° í™˜ê²½ì—ì„œì˜ UAV-ê¸°ë°˜ Semantic Communicationâ€“MEC í†µí•© í”„ë ˆì„ì›Œí¬ ì œì•ˆ, í˜¼í•© ì—°ì†â€“ì´ì‚° ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ T5D(DT3+DDQN) ê¸°ë°˜ DRL ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„, ì§€ëŠ¥í˜• ì¬ë¨¸ë¥¼ í¬í•¨í•œ ì ëŒ€ì  í•™ìŠµ êµ¬ì¡° ëª¨ë¸ë§, Semantic Communication ê´€ì ì—ì„œì˜ ì„±ëŠ¥ ì§€í‘œ ë° ì‹œìŠ¤í…œ ë¶„ì„ ì…ë‹ˆë‹¤. "}
    },
    {
        "inputs": {"question": "ë…¼ë¬¸ì—ì„œ ì‹œìŠ¤í…œì˜ ìµœì í™”ë¥¼ ìœ„í•´ ì‚¬ìš©í•œ ë‘ ê°€ì§€ í•µì‹¬ ë”¥ ê°•í™”í•™ìŠµ(DRL) ì•Œê³ ë¦¬ì¦˜ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?"},
        "outputs": {"answer": "Deep Q-Learning (DQL) ì•Œê³ ë¦¬ì¦˜ê³¼ Dueling Deep Q-Learning (Dueling DQL) ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. "}
    },
    {
        "inputs": {"question": "ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ì—ì„œ Dueling DQLì´ ê¸°ì¡´ DQL ë°©ì‹ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"},
        "outputs": {"answer": "Dueling DQLì€ ìƒíƒœ ê°€ì¹˜(Value function)ì™€ í–‰ë™ ì´ì (Advantage function)ì„ ë¶„ë¦¬í•˜ì—¬ ì¶”ì •í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ëª¨ë“  ìƒíƒœ-í–‰ë™ ìŒì„ íƒìƒ‰í•˜ì§€ ì•Šì•„ë„ í•™ìŠµì´ ê°€ëŠ¥í•˜ì—¬ ìˆ˜ë ´ ì†ë„ê°€ ë” ë¹ ë¥´ê³ , ë” ì•ˆì •ì ì¸ ë³´ìƒ(reward)ì„ ì–»ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤."}
    }
]

# -----------------------------------------------------------------------------
# 2. Kimnote RAG ì‹œìŠ¤í…œ ë˜í•‘ (Target Function)
# -----------------------------------------------------------------------------

def initialize_kimnote_app(pdf_path: str):
    print(f"Loading retriever from: {pdf_path}")
    retriever = build_retriever(pdf_path)
    if not retriever:
        raise ValueError("Retriever ìƒì„± ì‹¤íŒ¨. PDF ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    app = create_rag_graph(retriever)
    return app

# â˜… ì£¼ì˜: ì‹¤ì œ í‰ê°€í•  PDF íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš” â˜…
PDF_PATH = "./data/sample_paper.pdf" 
rag_app = None

if os.path.exists(PDF_PATH):
    rag_app = initialize_kimnote_app(PDF_PATH)
else:
    print(f"âš ï¸ ê²½ê³ : '{PDF_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def predict_kimnote(inputs: dict) -> dict:
    if rag_app is None:
        return {"output": "ì•±ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "contexts": []}

    question = inputs["question"]
    response_state = rag_app.invoke({"question": question})
    
    final_answer = response_state.get("generation", "")
    retrieved_docs = response_state.get("documents", [])
    
    return {
        "output": final_answer,
        "contexts": retrieved_docs
    }

# -----------------------------------------------------------------------------
# 3. ì§ì ‘ êµ¬í˜„í•œ í‰ê°€ ë¡œì§ (Custom Evaluators)
# -----------------------------------------------------------------------------

# 3.1 ë‹µë³€ ì •í™•ì„± í‰ê°€ (QA Correctness)
class CorrectnessScore(BaseModel):
    score: int = Field(description="ë‹µë³€ì˜ ì •í™•ì„± ì ìˆ˜ (1: ë¶€ì •í™• ~ 5: ë§¤ìš° ì •í™•)")
    reasoning: str = Field(description="ì ìˆ˜ ë¶€ì—¬ ì´ìœ ")

def evaluate_correctness(run, example):
    """
    ì •ë‹µ(Ground Truth)ê³¼ ì˜ˆì¸¡(Prediction)ì„ ë¹„êµí•˜ì—¬ ì •í™•ì„±ì„ 1~5ì ìœ¼ë¡œ í‰ê°€
    """
    prediction = run.outputs["output"]
    reference = example.outputs["answer"]
    input_question = example.inputs["question"]

    # ì±„ì ìš© í”„ë¡¬í”„íŠ¸
    prompt = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ê³µì •í•œ ì±„ì ê´€ì…ë‹ˆë‹¤. ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•œ 'ì‹¤ì œ ì •ë‹µ(Ground Truth)'ê³¼ AIê°€ ìƒì„±í•œ 'ì˜ˆì¸¡ ë‹µë³€(Prediction)'ì„ ë¹„êµí•˜ì—¬ í‰ê°€í•´ì£¼ì„¸ìš”.

[ì§ˆë¬¸]: {question}
[ì‹¤ì œ ì •ë‹µ]: {reference}
[ì˜ˆì¸¡ ë‹µë³€]: {prediction}

ì˜ˆì¸¡ ë‹µë³€ì´ ì‹¤ì œ ì •ë‹µì˜ í•µì‹¬ ì˜ë¯¸ë¥¼ ì˜ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ íŒë‹¨í•˜ì—¬ 1ì ì—ì„œ 5ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”.
1ì : ì™„ì „íˆ í‹€ë¦¼
3ì : ì¼ë¶€ ë§ìœ¼ë‚˜ ëˆ„ë½ë˜ê±°ë‚˜ ë¶€ì •í™•í•œ ë‚´ìš© ìˆìŒ
5ì : í•µì‹¬ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ í¬í•¨í•¨
"""
    )
    
    # êµ¬ì¡°í™”ëœ ì¶œë ¥ìœ¼ë¡œ ì ìˆ˜ ì¶”ì¶œ
    evaluator = prompt | eval_llm.with_structured_output(CorrectnessScore)
    result = evaluator.invoke({
        "question": input_question,
        "reference": reference,
        "prediction": prediction
    })

    return {
        "key": "correctness",
        "score": result.score / 5.0,  # 0~1 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
        "comment": result.reasoning
    }


# 3.2 ë¬¸ë§¥ ê¸°ë°˜ ì‚¬ì‹¤ì„± í‰ê°€ (Groundedness / Hallucination Check)
class GroundednessScore(BaseModel):
    is_grounded: str = Field(description="ë‹µë³€ì´ ë¬¸ë§¥ì— ê¸°ë°˜í–ˆëŠ”ì§€ ì—¬ë¶€ ('yes' or 'no')")
    reasoning: str = Field(description="íŒë‹¨ ì´ìœ ")

def evaluate_groundedness(run, example):
    """
    ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œ(Contexts)ì— ê¸°ë°˜í–ˆëŠ”ì§€(í™˜ê° ì—¬ë¶€) í‰ê°€
    """
    prediction = run.outputs["output"]
    contexts = run.outputs["contexts"]
    input_question = example.inputs["question"]
    
    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    context_str = "\n\n".join(contexts) if isinstance(contexts, list) else str(contexts)

    # ì±„ì ìš© í”„ë¡¬í”„íŠ¸
    prompt = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ 'í™˜ê°(Hallucination)'ì„ íƒì§€í•˜ëŠ” ê²€ì‚¬ê´€ì…ë‹ˆë‹¤.
AIì˜ ë‹µë³€ì´ ì œê³µëœ 'ì°¸ì¡° ë¬¸ì„œ(Context)'ì— ìˆëŠ” ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

[ì°¸ì¡° ë¬¸ì„œ]:
{context}

[AI ë‹µë³€]:
{prediction}

ë‹µë³€ì˜ ëª¨ë“  ë‚´ìš©ì´ ë¬¸ì„œì— ì˜í•´ ë’·ë°›ì¹¨ëœë‹¤ë©´ 'yes', ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ëƒˆë‹¤ë©´ 'no'ë¼ê³  ë‹µí•˜ì„¸ìš”.
"""
    )

    evaluator = prompt | eval_llm.with_structured_output(GroundednessScore)
    result = evaluator.invoke({
        "context": context_str,
        "prediction": prediction
    })
    
    # yes = 1ì  (í™˜ê° ì—†ìŒ), no = 0ì  (í™˜ê°)
    score = 1 if result.is_grounded.lower() == "yes" else 0

    return {
        "key": "groundedness",
        "score": score,
        "comment": result.reasoning
    }

# -----------------------------------------------------------------------------
# 4. LangSmith Dataset ìƒì„± ë° í‰ê°€ ì‹¤í–‰
# -----------------------------------------------------------------------------

def run_evaluation():
    client = Client()
    dataset_name = "KimNote_Evaluation_Dataset_V1"

    if not client.has_dataset(dataset_name=dataset_name):
        dataset = client.create_dataset(dataset_name=dataset_name)
        client.create_examples(
            inputs=[e["inputs"] for e in EVAL_DATASET],
            outputs=[e["outputs"] for e in EVAL_DATASET],
            dataset_id=dataset.id,
        )
        print(f"âœ… ë°ì´í„°ì…‹ '{dataset_name}' ìƒì„± ì™„ë£Œ.")
    else:
        print(f"â„¹ï¸ ê¸°ì¡´ ë°ì´í„°ì…‹ '{dataset_name}'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    print("ğŸš€ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (LangSmith ëŒ€ì‹œë³´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”)")
    
    results = evaluate(
        predict_kimnote,
        data=dataset_name,
        evaluators=[
            evaluate_correctness,
            evaluate_groundedness
        ],
        experiment_prefix="kimnote-custom-eval",
        metadata={"description": "KimNote RAG Evaluation with Custom LLM Judges"}
    )
    
    print("\nğŸ í‰ê°€ ì™„ë£Œ!")
    print(results)

if __name__ == "__main__":
    if rag_app:
        run_evaluation()
    else:
        print("âŒ ì‹¤í–‰ ì‹¤íŒ¨: PDF ê²½ë¡œë¥¼ í™•ì¸í•˜ê³  íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")