from typing import List, TypedDict

from langgraph.graph import StateGraph, END, START
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.tools.tavily_search import TavilySearchResults
from pydantic import BaseModel, Field

from models import LLM_MODEL

# -----------------------------------------------------------------------
# LangGraphì—ì„œ ìƒíƒœ(State)ë¥¼ í‘œí˜„í•  TypedDict ì •ì˜
# ê° ë…¸ë“œ í•¨ìˆ˜ëŠ” GraphStateë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê³ , GraphStateì˜ ë¶€ë¶„ì§‘í•©ì„ ë°˜í™˜
# -----------------------------------------------------------------------
class GraphState(TypedDict, total=False):
    question: str               # ì‚¬ìš©ì ì§ˆë¬¸ ì›ë¬¸
    documents: List[str]        # ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    generation: str             # LLMì´ ìƒì„±í•œ ë‹µë³€ í…ìŠ¤íŠ¸
    sub_queries: List[str]      # ì§ˆë¬¸ ë¶„í•´ ê²°ê³¼ë¡œ ìƒì„±ëœ í•˜ìœ„ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
    hallucination_status: str   # í™˜ê° ì—¬ë¶€ í‰ê°€ ê²°ê³¼ ('yes' ë˜ëŠ” 'no')
    hallucination_retries: int  # í™˜ê° ë°œìƒ ì‹œ ì¬ìƒì„±(re-generate) ì‹œë„ íšŸìˆ˜


# --- êµ¬ì¡°í™”ëœ ì¶œë ¥ ë°ì´í„° ëª¨ë¸ ---
# 03-OutputParser/01-PydanticOutputParser.ipynb ì°¸ê³  
# Pydanticì˜ BaseModelì„ ì •ì˜í•˜ê³ , LLMì— with_structured_output() ë©”ì„œë“œë¡œ JSON ìŠ¤í‚¤ë§ˆ ê°•ì œ 
# ë³€ê²½ì : binary_score í•„ë“œë¥¼ str íƒ€ì…ìœ¼ë¡œ ë³€ê²½ (ì›ë˜ëŠ” bool íƒ€ì…)
class GradeDocuments(BaseModel):
    # ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„± í‰ê°€ ìŠ¤í‚¤ë§ˆ
    binary_score: str = Field(
        description="ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆìœ¼ë©´ 'yes', ì—†ìœ¼ë©´ 'no'ë¡œ í‰ê°€"
    )
    # LLMì´ ì´ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ JSON í˜•íƒœë¡œ ì¶œë ¥ â†’ LangChainì´ íŒŒì‹±


class GradeHallucination(BaseModel):
    # ìƒì„±ëœ ë‹µë³€ì˜ í™˜ê° ì—¬ë¶€ í‰ê°€ ìŠ¤í‚¤ë§ˆ (Groundedness Check)
    binary_score: str = Field(
        description="ë‹µë³€ì´ ë¬¸ì„œì— ê¸°ë°˜í•˜ì—¬ ì‚¬ì‹¤ì ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©´ 'yes', ì•„ë‹ˆë©´ 'no'"
    )
    # ë‹µë³€ì´ ê·¼ê±° ë¬¸ì„œ(context)ì— ê¸°ë°˜í–ˆëŠ”ì§€ë¥¼ ì´ì§„(yes/no)ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ì§€í‘œ


# ------------------------------------------------------------------------------
# RAG + í’ˆì§ˆê´€ë¦¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ì •ì˜í•˜ëŠ” LangGraph ì•± ìƒì„± í•¨ìˆ˜
# retriever: ì´ë¯¸ build_retrieverì—ì„œ êµ¬ì„±ëœ ìµœì¢… retriever (Ensemble + Reranker)
# ------------------------------------------------------------------------------
def create_rag_graph(retriever):
    """Retrieverê°€ ì£¼ì…ëœ LangGraph ì•± ìƒì„±"""

    # --- ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---
    
    # 13-LangChain-Expression-Language/04-LCEL-Advanced.ipynbì˜ ê¸°ë³¸ ì²´ì¸ êµ¬ì„±
    def query_decomposition_node(state: GraphState) -> GraphState:
        """
        - ì…ë ¥: question
        - ì¶œë ¥: sub_queries (ì§ˆë¬¸ì„ ê²€ìƒ‰í•˜ê¸° ì¢‹ì€ nê°œì˜ í•˜ìœ„ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸),
                hallucination_retries ì´ˆê¸°í™”
        """
        
        question = state["question"]    # í˜„ì¬ ìƒíƒœì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
        
        # LLMì—ê²Œ "ê²€ìƒ‰í•˜ê¸° ì¢‹ì€ ê°œì˜ í•˜ìœ„ ì§ˆë¬¸"ìœ¼ë¡œ ë‚˜ëˆ„ë„ë¡ ì§€ì‹œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿*
        prompt = ChatPromptTemplate.from_template(
            "ì§ˆë¬¸ì„ ê²€ìƒ‰í•˜ê¸° ì¢‹ì€ nê°œì˜ í•œêµ­ì–´ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¦¬í•´ì¤˜. "
            "ê²°ê³¼ëŠ” ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•´.\nì§ˆë¬¸: {question}"
        )
        
        # promptë¥¼ StrOutputParserì„ í†µí•´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜*
        chain = prompt | LLM_MODEL | StrOutputParser()
        
        # ì‹¤ì œ ì§ˆë¬¸(q)ì„ ë„£ì–´ LLM ì‹¤í–‰*
        response = chain.invoke({"question": question})
        
        # LLMì´ ë°˜í™˜í•œ ì—¬ëŸ¬ ì¤„ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¤„ ë‹¨ìœ„ë¡œ split â†’ ê³µë°± ì œê±°*
        sub_queries = [q.strip() for q in response.split("\n") if q.strip()]
        
        # ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê¸°í™”
        return {"sub_queries": sub_queries,     # ë¶„í•´ëœ í•˜ìœ„ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
                "hallucination_retries": 0      # í™˜ê° ì¬ì‹œë„ ì¹´ìš´í„° ì´ˆê¸°í™”
                }

    # 10-Retriever
    def retrieval_node(state: GraphState) -> GraphState:
        """
        - ì…ë ¥: sub_queries
        - ê° í•˜ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ retriever.invoke(query)ë¥¼ í˜¸ì¶œí•œ í›„
          ëª¨ë“  ê²°ê³¼ ë¬¸ì„œ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ëª¨ìŒ
        - ì¶œë ¥: documents (ì¤‘ë³µ ì œê±°ëœ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸)
        """
        sub_queries = state.get("sub_queries", [])  # í•˜ìœ„ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (multi-query*)
        all_docs: List[str] = []                    # ëª¨ë“  ë¬¸ì„œ ë‚´ìš©ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™” 

        for q in sub_queries: #sub_queriesì˜ ê° í•˜ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë°˜ë³µ*
            # build_retrieverì—ì„œ ë§Œë“  Ensemble Retriever í˜¸ì¶œ*
            docs = retriever.invoke(q)
            for d in docs:
                # ê° Document ê°ì²´ì—ì„œ page_contentë§Œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                all_docs.append(d.page_content)

        # ê°™ì€ ë‚´ìš©ì˜ ë¬¸ì„œê°€ ì¤‘ë³µë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ setìœ¼ë¡œ í•œë²ˆ ì¤‘ë³µ ì œê±° í›„ ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ*
        unique_docs = list(set(all_docs))
        return {"documents": unique_docs}   # ìƒíƒœì— documents í•„ë“œë¡œ ì €ì¥


    # 12-RAG
    def grade_documents_node(state: GraphState) -> GraphState:
        """
        - ì…ë ¥: question, documents (ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸)
        - ê° ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ”ì§€ LLMìœ¼ë¡œ í‰ê°€
        - ê´€ë ¨ì„±ì´ "yes"ì¸ ë¬¸ì„œë§Œ í•„í„°ë§í•´ì„œ ë°˜í™˜
        """
        question = state["question"]
        documents = state.get("documents", [])
        
        # LLMì„ Structured Output ëª¨ë“œë¡œ ì‚¬ìš© (GradeDocuments ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì¶œë ¥ ê°•ì œ)*
        structured_llm_grader = LLM_MODEL.with_structured_output(GradeDocuments)
        
        # LLMì—ê²Œ í‰ê°€ìì˜ ì—­í• ê³¼ ê¸°ì¤€ì„ ì„¤ëª…*
        system_msg = (
            "ë‹¹ì‹ ì€ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ì±„ì ìì…ë‹ˆë‹¤. "
            "ë¬¸ì„œì— ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í‚¤ì›Œë“œë‚˜ ì˜ë¯¸ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ 'yes'ë¡œ í‰ê°€í•˜ì„¸ìš”. ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ 'no'ë¡œ í‰ê°€í•˜ì„¸ìš”."
            "ì—„ê²©í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤. ê´€ë ¨ì„±ì´ ì¡°ê¸ˆì´ë¼ë„ ìˆë‹¤ë©´ 'yes'ë¥¼ ì£¼ì„¸ìš”."
        )
        
        # ì‹¤ì œ ì§ˆë¬¸ê³¼ ë‹¨ì¼ ë¬¸ì„œë¥¼ ë„£ì–´ í˜¸ì¶œ*
        grade_prompt = ChatPromptTemplate.from_messages(
            [("system", system_msg), ("human", "ì§ˆë¬¸: {question}\n\në¬¸ì„œ: {document}")]
        )
        
        # í”„ë¡¬í”„íŠ¸ + LLM(êµ¬ì¡°í™” ì¶œë ¥) ì²´ì¸ êµ¬ì„±
        retrieval_grader = grade_prompt | structured_llm_grader
        
        filtered_docs = []  # ê´€ë ¨ì„±ì´ yesë¡œ í‰ê°€ëœ ë¬¸ì„œë§Œ ëª¨ì„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        for doc in documents: 
             # ê° ë¬¸ì„œì— ëŒ€í•´ LLM í‰ê°€ ìˆ˜í–‰ *
            score = retrieval_grader.invoke({"question": question, "document": doc})
            # binary_scoreê°€ "yes"ì¼ ë•Œë§Œ ë¬¸ì„œ ì±„íƒ
            if score.binary_score.lower() == "yes":
                filtered_docs.append(doc)
        
        return {"documents": filtered_docs} # ìƒíƒœì— í•„í„°ë§ëœ documentsë§Œ ë‹¤ì‹œ ì €ì¥


    # 12-RAG/03-Conversation-With-History.ipynbì˜ generate ì²´ì¸ ì°¸ê³ 
    def generate_node(state: GraphState) -> GraphState:
        """
        - ì…ë ¥: question, documents, hallucination_retries
        - ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ contextë¡œ í•©ì³ LLMì— ì „ë‹¬í•˜ê³ ,
        ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ í•™ìˆ ì /ì „ë¬¸ì  ë‹µë³€ ì œê³µ
        - í™˜ê° ì¬ì‹œë„ê°€ ìˆëŠ” ê²½ìš°, ë” ì—„ê²©í•˜ê²Œ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ë¼ëŠ” guidance ì¶”ê°€
        - ì¶œë ¥: generation (ìµœì¢… ë‹µë³€ í…ìŠ¤íŠ¸)
        """
        question = state["question"]
        documents = state.get("documents", [])
        context = "\n\n".join(documents)
        retries = state.get("hallucination_retries", 0)
        
        # ì§ˆë¬¸ ìœ í˜• ê°ì§€ ë° ì ì‘í˜• guidance ìƒì„±
        guidance = _generate_adaptive_guidance(question, retries)

        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ì ì‘í˜• í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        prompt = ChatPromptTemplate.from_template(
            "ë‹¹ì‹ ì€ ë…¼ë¬¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”.\n"
            "ì•„ë˜ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ í•™ìˆ ì ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.\n"
            "êµ¬ì²´ì ì¸ ì•Œê³ ë¦¬ì¦˜ëª…ì´ë‚˜ ìˆ«ìëŠ” ì •í™•í•˜ê²Œ ê¸°ì–µí•´ì¤˜.\n"
            "ëª¨ë“  ì£¼ì¥ì€ ì œì‹œëœ ë¬¸ì„œì˜ ê·¼ê±°ë¡œ ë’·ë°›ì¹¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
            "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´ ì§€ì–´ë‚´ì§€ ë§ê³  'ë¬¸ì„œì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.\n"
            "{guidance}\n\n"
            "[ë¬¸ì„œ]\n{context}\n\n[ì§ˆë¬¸]\n{question}"
        )
        
        chain = prompt | LLM_MODEL | StrOutputParser()
        generation = chain.invoke({"context": context, "question": question, "guidance": guidance})
        
        return {"generation": generation}


    def _generate_adaptive_guidance(question: str, retries: int) -> str:
        """
        ì§ˆë¬¸ ìœ í˜•ì„ ë¶„ì„í•˜ì—¬ ì ì‘í˜• ê°€ì´ë“œ ë©”ì‹œì§€ ìƒì„±
        
        ì§ˆë¬¸ ìœ í˜•:
        1. ì •ì˜/ê°œë… ì§ˆë¬¸ (what/ì •ì˜)
        2. ë¹„êµ/ëŒ€ì¡° ì§ˆë¬¸ (vs/ì°¨ì´/ë¹„êµ)
        3. ì›ì¸/ê²°ê³¼ ì§ˆë¬¸ (why/cause/ì˜í–¥)
        4. ë°©ë²•ë¡  ì§ˆë¬¸ (how/ë°©ë²•/í”„ë¡œì„¸ìŠ¤)
        5. í‰ê°€/ë¶„ì„ ì§ˆë¬¸ (í‰ê°€/ë¶„ì„/ì¥ë‹¨ì )
        """
        
        question_lower = question.lower()
        
        # ì¬ì‹œë„ ì‹œ ê³µí†µ ê¸°ë³¸ ê°€ì´ë“œ
        base_guidance = ""
        if retries > 0:
            base_guidance = "âš ï¸ ì´ì „ ë‹µë³€ì´ ë¶€ì •í™•í–ˆìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ ë¬¸ì„œì˜ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì—¬ ì •í™•íˆ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”. ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ í•´ì„ì´ë‚˜ ì¶”ë¡ ì€ í”¼í•˜ì„¸ìš”.\n"
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ê°€ì´ë“œ
        if any(keyword in question_lower for keyword in ["ì •ì˜", "ëœ»", "ë¬´ì—‡", "what is", "definition"]):
            # ì •ì˜/ê°œë… ì§ˆë¬¸
            return (
                base_guidance +
                "ğŸ“Œ ë‹µë³€ í˜•ì‹:\n"
                "1) í•µì‹¬ ì •ì˜ë¥¼ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì œì‹œ\n"
                "2) í•µì‹¬ íŠ¹ì„± ë˜ëŠ” ìš”ì†Œ (ìµœëŒ€ 3ê°œ) ì—´ê±°\n"
                "3) í•™ë¬¸ ë¶„ì•¼ì—ì„œ í•´ë‹¹ ê°œë…ì˜ ìœ„ì¹˜ ë˜ëŠ” ì¤‘ìš”ì„± ì„¤ëª…\n"
                "4) ì‹¤ì œ ì‚¬ë¡€ë‚˜ ì ìš© ë¶„ì•¼ (ë¬¸ì„œì— ìˆëŠ” ê²½ìš°ë§Œ)"
            )
        
        elif any(keyword in question_lower for keyword in ["ë¹„êµ", "vs", "ì°¨ì´", "ë‹¤ë¥´", "ì°¨ì´ì ", "difference", "compare", "contrast"]):
            # ë¹„êµ/ëŒ€ì¡° ì§ˆë¬¸
            return (
                base_guidance +
                "ğŸ“Š ë‹µë³€ í˜•ì‹:\n"
                "1) ë¹„êµ ëŒ€ìƒì˜ ì •ì˜ ê°„ë‹¨íˆ ì œì‹œ\n"
                "2) í‘œ í˜•ì‹ ë˜ëŠ” í•­ëª©ë³„ë¡œ ì£¼ìš” ì°¨ì´ì  ëª…ì‹œ (ì˜ˆ: ì •ì˜, íŠ¹ì„±, ìš©ë„, ì¥ì , í•œê³„ ë“±)\n"
                "3) ê° ì°¨ì´ê°€ ì˜ë¯¸í•˜ëŠ” ë°”ë¥¼ í•™ìˆ ì ìœ¼ë¡œ í•´ì„\n"
                "4) ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ëŠ ê²ƒì„ ì‚¬ìš©í•˜ëŠ”ì§€ (ë¬¸ì„œ ê¸°ë°˜)"
            )
        
        elif any(keyword in question_lower for keyword in ["ì™œ", "ì›ì¸", "ì´ìœ ", "why", "cause", "reason", "ì˜í–¥", "effect", "impact"]):
            # ì›ì¸/ê²°ê³¼/ì˜í–¥ ì§ˆë¬¸
            return (
                base_guidance +
                "ğŸ”— ë‹µë³€ í˜•ì‹:\n"
                "1) ì¸ê³¼ê´€ê³„ë¥¼ ëª…í™•í•˜ê²Œ ì„¤ëª…\n"
                "2) ì›ì¸ â†’ ë©”ì»¤ë‹ˆì¦˜ â†’ ê²°ê³¼ì˜ ë‹¨ê³„ë³„ ë…¼ë¦¬ ì „ê°œ\n"
                "3) ì¤‘ê°„ ë‹¨ê³„ì˜ ì„¸ë¶€ ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª… (ë¬¸ì„œì—ì„œ ì§€ì›ë˜ëŠ” ê²½ìš°)\n"
                "4) ì˜ˆì™¸ ìƒí™©ì´ë‚˜ ì¡°ê±´ë¶€ ê²°ê³¼ ì–¸ê¸‰ (ìˆë‹¤ë©´)\n"
                "5) ê´€ë ¨ ì—°êµ¬ë‚˜ ì´ë¡ ì  ë°°ê²½ (ë¬¸ì„œì— ìˆë‹¤ë©´)"
            )
        
        elif any(keyword in question_lower for keyword in ["ì–´ë–»ê²Œ", "ë°©ë²•", "í”„ë¡œì„¸ìŠ¤", "ì ˆì°¨", "ë‹¨ê³„", "how", "process", "procedure", "step"]):
            # ë°©ë²•ë¡ /í”„ë¡œì„¸ìŠ¤ ì§ˆë¬¸
            return (
                base_guidance +
                "ğŸ”§ ë‹µë³€ í˜•ì‹:\n"
                "1) ì „ì²´ í”„ë¡œì„¸ìŠ¤ì˜ ê°œìš” ì œì‹œ\n"
                "2) ê° ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª… (ë‹¨ê³„ë³„ ì œëª© í¬í•¨)\n"
                "3) ê° ë‹¨ê³„ë³„ ì£¼ì˜ì , ì¡°ê±´, ë˜ëŠ” í•„ìˆ˜ ìš”ì†Œ\n"
                "4) ë‹¨ê³„ ê°„ì˜ ì—°ê³„ì„±ê³¼ ì˜ì¡´ì„±\n"
                "5) ê²°ê³¼ ê²€ì¦ ë°©ë²• ë˜ëŠ” ì„±ê³µ ê¸°ì¤€ (ìˆë‹¤ë©´)"
            )
        
        elif any(keyword in question_lower for keyword in ["í‰ê°€", "ë¶„ì„", "ì¥ì ", "ë‹¨ì ", "ì¥ë‹¨ì ", "í‰ê°€", "assess", "analysis", "advantage", "disadvantage", "strength", "weakness"]):
            # í‰ê°€/ë¶„ì„/ì¥ë‹¨ì  ì§ˆë¬¸
            return (
                base_guidance +
                "âš–ï¸ ë‹µë³€ í˜•ì‹:\n"
                "1) í‰ê°€ ê¸°ì¤€ ë˜ëŠ” ë¶„ì„ í‹€ ëª…ì‹œ\n"
                "2) ê¸ì •ì  ì¸¡ë©´ (ê·¼ê±° ì œì‹œ)\n"
                "3) ë¶€ì •ì  ì¸¡ë©´ ë˜ëŠ” í•œê³„ (ê·¼ê±° ì œì‹œ)\n"
                "4) ì¤‘ë¦½ì  ê´€ì ì—ì„œ ìƒí™©ë³„ ì ìš© ê°€ëŠ¥ì„±\n"
                "5) í•™ìˆ ì  í‰ê°€ ë˜ëŠ” í˜„ì¥ ì „ë¬¸ê°€ì˜ ê²¬í•´ (ë¬¸ì„œ ê¸°ë°˜)"
            )
        
        else:
            # ê¸°íƒ€ ì¼ë°˜ì ì¸ ì§ˆë¬¸
            return (
                base_guidance +
                "ğŸ“ ë‹µë³€ í˜•ì‹:\n"
                "1) ì§ˆë¬¸ì˜ í•µì‹¬ ê°œë…ì„ ëª…í™•íˆ ì •ì˜\n"
                "2) ì¤‘ìš” ë‚´ìš©ì„ ë…¼ë¦¬ì  ìˆœì„œëŒ€ë¡œ ì „ê°œ\n"
                "3) ê° ì£¼ì¥ì€ ë¬¸ì„œì˜ êµ¬ì²´ì  ê·¼ê±°ë¡œ ë’·ë°›ì¹¨\n"
                "4) í•„ìš”ì‹œ ë„í‘œ, ì˜ˆì‹œ, ë˜ëŠ” ì‚¬ë¡€ í¬í•¨\n"
                "5) ê²°ë¡ ì€ ëª…í™•í•˜ê³  ì¬í˜„ ê°€ëŠ¥í•œ í‘œí˜„ìœ¼ë¡œ ë§ˆë¬´ë¦¬"
            )

    # 12-RAG
    def web_search_node(state: GraphState) -> GraphState:
        """
        - ì…ë ¥: question
        - Tavily ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ ì›¹ì—ì„œ k=3ê°œì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´
        - ê° ê²°ê³¼ì˜ contentë§Œ ì¶”ì¶œí•˜ì—¬ documents ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        - ì˜ˆì™¸ ë°œìƒ ì‹œ, ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ documentsë¡œ ë°˜í™˜
        """
        try:
            tool = TavilySearchResults(k=3)                     # Tavily ê²€ìƒ‰ ë„êµ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            docs = tool.invoke({"query": state["question"]})    # ì›¹ ê²€ìƒ‰ ì‹¤í–‰
            web_content = [d["content"] for d in docs]          # ê²°ê³¼ì—ì„œ contentë§Œ ì¶”ì¶œ
            return {"documents": web_content}
        except Exception:
            return {"documents": ["ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]}    # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œì—ë„ ê·¸ë˜í”„ê°€ ì£½ì§€ ì•Šê³  fallback í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    
    
    # 12-RAG / 16-Evaluations
    def hallucination_check_node(state: GraphState) -> GraphState:
        """
        - ì…ë ¥: documents (ê·¼ê±° ë¬¸ì„œ), generation (LLM ë‹µë³€),
                hallucination_retries
        - LLMì„ ì‚¬ìš©í•´ ë‹µë³€ì´ ë¬¸ì„œì— ê·¼ê±°í–ˆëŠ”ì§€ í‰ê°€
        - í™˜ê°ì´ë©´ retriesë¥¼ 1 ì¦ê°€
        - ì¶œë ¥: hallucination_status ('yes' or 'no'), hallucination_retries
        """
        documents = state.get("documents", [])
        generation = state["generation"]        # generate_nodeì—ì„œ ìƒì„±ëœ ë‹µë³€
        context = "\n\n".join(documents)        # ê·¼ê±°ë¡œ ì‚¬ìš©í•  ë¬¸ì„œ context
        
        # í˜„ì¬ê¹Œì§€ì˜ ì¬ì‹œë„ íšŸìˆ˜
        retries = state.get("hallucination_retries", 0)

         # í™˜ê° íŒë³„ìš© LLMì„ êµ¬ì¡°í™” ì¶œë ¥ ëª¨ë“œë¡œ ì‚¬ìš©*
        structured_llm_grader = LLM_MODEL.with_structured_output(GradeHallucination)
        system_msg = (
            "ë‹¹ì‹ ì€ AI ë‹µë³€ì´ ì°¸ì¡° ë¬¸ì„œ(Context)ì— ê¸°ë°˜í–ˆëŠ”ì§€ ê²€ì¦í•˜ëŠ” ì±„ì ìì…ë‹ˆë‹¤. "
            "ë‹µë³€ì´ ë¬¸ì„œì˜ ë‚´ìš©ìœ¼ë¡œ ë’·ë°›ì¹¨ëœë‹¤ë©´ 'yes', ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ëƒˆë‹¤ë©´ 'no'ë¡œ í‰ê°€í•˜ì„¸ìš”."
            "ë§Œì•½ ë¬¸ì„œì— ì •ë³´ê°€ ì—†ì–´ì„œ 'ëª¨ë¥¸ë‹¤'ê³  ë‹µí•œ ê²½ìš°ì—ë„ 'yes'(ì‚¬ì‹¤ ê¸°ë°˜)ë¡œ í‰ê°€í•˜ì„¸ìš”."
        )
        
        # context(ë¬¸ì„œë“¤)ì™€ generation(ë‹µë³€)ì„ í•¨ê»˜ ë³´ì—¬ì£¼ê³  íŒë‹¨ì„ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸*
        grade_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_msg),
                ("human", "ë¬¸ì„œ: {context}\n\në‹µë³€: {generation}")
            ]
        )
        hallucination_grader = grade_prompt | structured_llm_grader
        
        # LLMì—ê²Œ contextì™€ generationì„ ë„˜ê²¨ í™˜ê° ì—¬ë¶€ íŒë‹¨ ì‹¤í–‰*
        score = hallucination_grader.invoke({"context": context, "generation": generation})
        
        status = score.binary_score.lower() # "yes" ë˜ëŠ” "no"*
        
        # í™˜ê°ìœ¼ë¡œ íŒì •ëœ ê²½ìš° ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€*
        if status == "no":
            retries += 1
            
        return {"hallucination_status": status, "hallucination_retries": retries}   # ìƒíƒœì— í™˜ê° ì—¬ë¶€ì™€ ìµœì‹  ì¬ì‹œë„ ì¹´ìš´í„°ë¥¼ ì €ì¥


    # -------------------------------------------------------------------
    #                       ë¼ìš°íŒ… í•¨ìˆ˜ë“¤ (ì¡°ê±´ ë¶„ê¸° ë¡œì§)
    # -------------------------------------------------------------------
    def decide_retrieval_route(state: GraphState) -> str:
        """
        grade ë…¸ë“œ ì´í›„ì˜ ë¶„ê¸° ê²°ì •
        - documentsê°€ ë¹„ì–´ ìˆìœ¼ë©´: web_search ë…¸ë“œë¡œ*
        - documentsê°€ ìˆìœ¼ë©´: generate ë…¸ë“œë¡œ*
        """
        documents = state.get("documents", [])
        if not documents:
            return "web_search"
        return "generate"

    def decide_hallucination_route(state: GraphState) -> str:
        """
        hallucination_check ë…¸ë“œ ì´í›„ì˜ ë¶„ê¸° ê²°ì •
        1) status == 'yes' (í™˜ê° ì•„ë‹˜)          -> END
        2) status == 'no' & retries > 3         -> END (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        3) status == 'no' & retries <= 3        -> generate (ì¬ì‹œë„)
        """
        status = state.get("hallucination_status", "yes")
        retries = state.get("hallucination_retries", 0)
        
        if status == "yes":
            return "end"    # ENDë¡œ ê°€ëŠ” ë¼ë²¨
        elif retries > 3:
            return "end"    # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í•˜ë©´ ê°•ì œ ì¢…ë£Œ (ë˜ëŠ” web_searchë¡œ ë³´ë‚´ëŠ” ê²ƒë„ ê°€ëŠ¥)
        else:
            return "generate"   # í™˜ê°ì´ì§€ë§Œ ì•„ì§ ì¬ì‹œë„ ê°€ëŠ¥ â†’ ë‹¤ì‹œ generate ë…¸ë“œë¡œ ë£¨í”„


    # -------------------------------------------------------------------
    #                              ê·¸ë˜í”„ ì¡°ë¦½
    # -------------------------------------------------------------------
    # GraphState íƒ€ì…ì„ ìƒíƒœë¡œ ì‚¬ìš©í•˜ëŠ” StateGraph ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    # 17-LangGraph
    workflow = StateGraph(GraphState)
    
    # ê° ë…¸ë“œë¥¼ ê·¸ë˜í”„ì— ë“±ë¡ (ë…¸ë“œ ì´ë¦„, í•¨ìˆ˜)
    workflow.add_node("decompose", query_decomposition_node)
    workflow.add_node("retrieve", retrieval_node)
    workflow.add_node("grade", grade_documents_node)
    workflow.add_node("generate", generate_node)
    workflow.add_node("web_search", web_search_node)
    workflow.add_node("hallucination_check", hallucination_check_node)

    # ê¸°ë³¸ ì—£ì§€ êµ¬ì„±: START â†’ decompose â†’ retrieve â†’ grade
    workflow.add_edge(START, "decompose")
    workflow.add_edge("decompose", "retrieve")
    workflow.add_edge("retrieve", "grade")
    
    # grade ì´í›„ ë¶„ê¸°:
    # - decide_retrieval_route(state)ê°€ "web_search"ë¥¼ ë°˜í™˜í•˜ë©´ web_search ë…¸ë“œë¡œ
    # - "generate"ë¥¼ ë°˜í™˜í•˜ë©´ generate ë…¸ë“œë¡œ
    workflow.add_conditional_edges(
        "grade",
        decide_retrieval_route,
        {"web_search": "web_search", "generate": "generate"},
    )
    
    # web_search ì´í›„ì—ëŠ” ë¬´ì¡°ê±´ generate ë…¸ë“œë¡œ ì´ë™
    workflow.add_edge("web_search", "generate")
    
    # generate ì´í›„ì—ëŠ” hallucination_check ë…¸ë“œë¡œ ì´ë™
    workflow.add_edge("generate", "hallucination_check")
    
    # hallucination_check ì´í›„ ë¶„ê¸°:
    # - decide_hallucination_route(state)ê°€ "end" â†’ END
    # - "generate" â†’ ë‹¤ì‹œ generate ë…¸ë“œë¡œ ë£¨í”„
    workflow.add_conditional_edges(
        "hallucination_check",
        decide_hallucination_route,
        {
            "end": END,
            "generate": "generate"
        }
    )

    return workflow.compile()